% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/build.downscalingBN.R
\name{build.downscalingBN}
\alias{build.downscalingBN}
\title{Downscale DISCRETE climate data using Bayesian Networks.}
\usage{
build.downscalingBN(data, structure.learning.algorithm = "hc",
  structure.learning.args.list = list(), param.learning.method = "bayes",
  forbid.GG = TRUE, forbid.DD = FALSE, forbid.DtoG = TRUE,
  dynamic = FALSE, epochs = 2, remove.past.G = TRUE,
  keep.dynamic.distance = TRUE, forbid.backwards = FALSE,
  forbid.past.dynamic.GD = TRUE, forbid.dynamic.GG = TRUE,
  forbid.past.DD = TRUE, structure.learning.steps = 1,
  fix.intermediate = TRUE, structure.learning.algorithm2 = NULL,
  structure.learning.args.list2 = list(),
  structure.learning.algorithm3 = NULL,
  structure.learning.args.list3 = list(), return.intermediate = FALSE,
  output.marginals = TRUE, compile.junction = TRUE, parallelize = FALSE,
  n.cores = NULL, cluster.type = "FORK")
}
\arguments{
\item{data}{Expects output from \code{\link[BNdownscaleR]{prepare_predictors.forBN}}.}

\item{structure.learning.algorithm}{Algorithm used to perform structure learning, with name as text. Supports all the score-based,
constraint-based  and hybrid bayesian network algorithms from \code{\link[bnlearn]{bnlearn}}.
Refer to \code{Details} for a list of supported algorithms.}

\item{structure.learning.args.list}{List of arguments passed to structure.learning.algorithm, in particular distance argument if
local learning is used. Refer to \code{\link[bnlearn]{bnlearn}} for the specific options.}

\item{param.learning.method}{Either "bayes" or "mle", passed to learn the parameters of the built network structure from \code{data}.}

\item{forbid.GG}{Arcs between grid or G nodes will be forbidden.}

\item{forbid.DD}{Arcs between local, i.e. station or D nodes, will be forbidden.}

\item{dynamic}{Set to TRUE to use Dynamic Bayesian Networks. See \code{Details}.}

\item{epochs}{Number of epochs to consider for Dynamic Bayesian Networks.}

\item{remove.past.G}{When \code{dynamic = TRUE} Set to TRUE to remove the past G nodes.}

\item{keep.dynamic.distance}{When \code{dynamic = TRUE} and local learning is employed, if set to TRUE it will use its corresponding
distance (See \code{Details}) between nodes from diferent epochs.}

\item{forbid.backwards}{When \code{dynamic = TRUE}, set to TRUE to forbid arcs going back in time.}

\item{forbid.past.dynamic.GD}{When \code{dynamic = TRUE}, set to TRUE to forbid arcs in the form G->D or D->G between different epochs.}

\item{forbid.dynamic.GG}{When \code{dynamic = TRUE} and \code{remove.past.G = FALSE}, set to TRUE to forbid arcs in the form G-G in the
past epochs.}

\item{forbid.past.DD}{When \code{dynamic = TRUE}, set to TRUE to forbid arcs in the form D-D in the past epochs.}

\item{structure.learning.steps}{It is used to perform structure learning in up to three steps. Refer to \code{Details}.
\itemize{
 \item \code{1} or \code{NULL} (Default) 1 step
 \item \code{2} or \code{c("local", "global")} If \code{dynamic = FALSE} learn first a DAG for D nodes, then inject G nodes. If \code{dynamic = TRUE} it
 equals c("local-global", "past")
 \item \code{3} Equals c("local", "global", "past")
 \item \code{c("local-global", "past")} or \code{c("global-local", "past")} Learn first DAG for D and G nodes, then inject past nodes.
 \item \code{c("local", "global-past")} or \code{c("local", "past-global")} Learn first DAG for D nodes, then inject past and G nodes.
 \item \code{c("local-past", "global")} or \code{c("past-local", "global")} Learn first DAG for D and past nodes, then inject G nodes.
 \item \code{c("local", "global", "past")} Learn first DAG for D nodes, then inject G nodes, then inject past nodes.
 \item \code{c("local", "past", "global")} Learn first DAG for D nodes, then inject past nodes, then inject G nodes.
}
Note that only first two options are valid when \code{dynamic = FALSE}}

\item{fix.intermediate}{Set to TRUE to forbid the creation of new arcs in the next steps for already built DAGs. See \code{Details}.
\code{structure.learning.algorithm2} and \code{structure.learning.args.list2}. See \code{Details}.}

\item{structure.learning.algorithm2}{Same as structure.learning.algorithm for the second step if \code{structure.learning.steps} is
employed. Ignored otherwise.}

\item{structure.learning.args.list2}{Same as structure.learning.args.list for the second step if \code{structure.learning.steps} is
employed. Ignored otherwise.}

\item{structure.learning.algorithm3}{Same as structure.learning.algorithm for the third step if \code{structure.learning.steps} with
3 steps is employed. Ignored otherwise.}

\item{structure.learning.args.list3}{Same as structure.learning.args.list for the third step if \code{structure.learning.steps} with
3 steps is employed. Ignored otherwise.
See \code{Details}.}

\item{return.intermediate}{Add the intermediate DAGs to the output, as $intermediateDBN1 and $intermediateDBN2 (if any) if
\code{structure.learning.steps} is employed.}

\item{output.marginals}{Compute and output Marginal Probability distribution Tables. Setting this to \code{FALSE} will force
\code{prediction.type = "probabilities"} in  \code{downscale.BN()}.}

\item{compile.junction}{Compile the junction from BN.fit to compute probabilities. Can be set to FALSE,
in which case it will be computed at the training stage in \code{downscale.BN()}.}

\item{parallelize}{Set to \code{TRUE} for parallelization. Refer to the \code{\link[parallel]{parallel}} and see \code{Details}.}

\item{n.cores}{When \code{parallelize = TRUE}, number of threads to be used, will use detectCores()-1 if not set.}

\item{cluster.type}{Either "PSOCK" or "FORK". Use the former under Windows systems, refer to \code{\link[parallel]{parallel}}
package.}
}
\value{
An object of type DBN, which contains, in particular, the Bayesian Network.
}
\description{
Downscale discrete data to local scales by means of Bayesian Networks.
}
\details{
\strong{Structure Learning Algorithms}
Use \code{structure.learning.algorithm}
Currently it DOES NOT support local discovery algorithms, expect malfuncion if used.
List of supported algorithms:
\code{"hc"}, \code{"tabu"} (score-based), \code{"gs"}, \code{"iamb"}, \code{"fast.iamb"}, \code{"inter.iamb"} (constraint-based),
\code{"mmhc"}, \code{"rsmax2"} (hybrid).
Check their corresponding parameters in \code{\link[bnlearn]{bnlearn}}, arguments may be passed to the algorithm through
the parameter structure.learning.args.list. Do not forget to set the distance argument in \code{structure.learning.args.list} for
local learning.

\strong{Two or Three Step Learning}
\itemize{
\item \code{structure.learning.steps} allows to build separate DAGs for each set of nodes. Note that by employing the three
\code{structure.learning.algorithm}, \code{structure.learning.algorithm2}, \code{structure.learning.algorithm3} arguments and their
corresponding \code{structure.learning.args.list*} counterparts, many different configurations can be used for the structure learning
process, e.g. by using grow-shrink for D nodes with distance set to 1, then injecting the left nodes using hill-climbing without distance
restriction.
\item \code{fix.intermediate}, if set to \code{TRUE}, will forbid the creation of new arcs between nodes that were present in the previous
learning step. E.g. if \code{structure.learning.steps = c("local", "global\-past")}, no new arcs between D nodes will be created in the
second step, as the first DAG will be considered finished. If set to \code{FALSE}, the previous step DAG will be kept, but the next
learning algorithm could create new arcs between D nodes over the first one.
}
\strong{Aditional details}
\code{output.marginals} and \code{compile.junction} are useful to save time if the user only intends to visualize the DAG.
\code{whitelist} and \code{blacklist} arguments can be passed to structure.learning.args.list, but beware of the naming convention,
it is best to use \code{plotDBN()} first with a dummy network.
}
\examples{
# Loading predictors
}
\author{
MN Legasa
}
