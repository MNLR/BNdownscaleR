% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/build.downscalingBN.R
\name{build.downscalingBN}
\alias{build.downscalingBN}
\title{Downscale DISCRETE climate data using Bayesian Networks.}
\usage{
build.downscalingBN(data, structure.learning.algorithm = "hc",
  structure.learning.args.list = list(), param.learning.method = "bayes",
  forbid.global.arcs = TRUE, forbid.local.arcs = FALSE, dynamic = FALSE,
  epochs = 2, only.present.G = TRUE, forbid.backwards = FALSE,
  forbid.dynamic.GD = TRUE, forbid.dynamic.global.arcs = TRUE,
  forbid.past.DD = TRUE, structure.learning.steps = NULL,
  structure.learning.algorithm2 = NULL,
  structure.learning.args.list2 = list(), return.first = FALSE,
  output.marginals = TRUE, compile.junction = TRUE, parallelize = FALSE,
  n.cores = NULL, cluster.type = "PSOCK")
}
\arguments{
\item{data}{Expects output from \code{\link[BNdownscaleR]{prepare_predictors.forBN}}.}

\item{structure.learning.algorithm}{Algorithm used to perform structure learning, with name as text. Supports all the score-based,
constraint-based  and hybrid bayesian network algorithms from \code{\link[bnlearn]{bnlearn}} and their *.local counterparts.
Refer to \code{Details} for a list of supported algorithms.}

\item{structure.learning.args.list}{List of arguments passed to structure.learning.algorithm, in particular distance argument if local learning
is used. Refer to \code{\link[bnlearn]{bnlearn}} for the specific options.}

\item{param.learning.method}{Either "bayes" or "mle", passed to learn the parameters of the built network structure from \code{data}.}

\item{forbid.global.arcs}{Arcs between grid nodes will be forbidden.}

\item{forbid.local.arcs}{Arcs between local, i.e. station nodes, will be forbidden.
Will be used in second step if two.step is set to TRUE. See \code{Details}.}

\item{structure.learning.algorithm2}{Same as structure.learning.algorithm for the global injection process,
ignored if two.step is set to FALSE. See \code{Details}.}

\item{structure.learning.args.list2}{Same as structure.learning.args.list for the global injection process,
ignored if two.step is set to FALSE. See \code{Details}.}

\item{output.marginals}{Compute and output Marginal Probability distribution Tables. Setting this to \code{FALSE} will force
\code{prediction.type = "probabilities"} in  downscale.BN().}

\item{compile.junction}{Compile the junction from BN.fit to compute probabilities. Can be set to FALSE,
in which case it will be computed at the training stage.}

\item{parallelize}{Set to \code{TRUE} for parallelization. Refer to the \code{\link[parallel]{parallel}} and see \code{Details}.}

\item{n.cores}{When \code{parallelize = TRUE}, number of threads to be used, will use detectCores()-1 if not set.}

\item{cluster.type}{Either "PSOCK" or "FORK". Use the former under Windows systems, refer to \code{\link[parallel]{parallel}}
package.}

\item{two.step}{Learn first a local bayesian network, i.e. just for the stations, then inject global (grid) nodes. See arguments
\code{structure.learning.algorithm2} and \code{structure.learning.args.list2}. See \code{Details}.}
}
\value{
An object of type DBN, which contains, in particular, the Bayesian Network.
}
\description{
Downscale discrete data to local scales by means of Bayesian Networks.
}
\details{
\strong{Structure Learning Algorithms}
Use \code{structure.learning.algorithm}
Currently it DOES NOT support local discovery algorithms, expect malfuncion if used.
List of supported algorithms:
"hc", "tabu" (score-based), "gs", "iamb", "fast.iamb", "inter.iamb" (constraint-based),  "mmhc", "rsmax2" (hybrid).
Use "*.local" to perform the local learning counterpart, e.g. "hc.local", "fast.iamb.local"...
Check their corresponding parameters in \code{\link[bnlearn]{bnlearn}}, arguments may be passed to the algorithm through
the parameter structure.learning.args.list. Do not forget to set the distance argument in \code{structure.learning.args.list} for
 *.local learning.

\strong{Two Step Learning}
When \code{two.step = TRUE}, an independent DAG (Directed Acyclic Graph) will be built for the local (i.e. grid or predictand)
nodes, then a second graph, containing the first as a subgraph and with the grid (predictor) nodes, will be learnt.
Note:
\itemize{
\item First step uses parameters \code{structure.learning.algorithm} and \code{structure.learning.args.list} are used for the
construction of the first DAG. \code{forbid.global.arcs} and \code{forbid.local.arcs} are ignored at this stage.
\item Global injection step uses \code{structure.learning.algorithm2} and \code{structure.learning.args.list2} for learning the
DAG structure. At this stage, \code{forbid.global.arcs = TRUE} will forbid arcs between grid (predictor) nodes, whereas
\code{forbid.local.arcs} will forbid the creation of new arcs between local (predictand) nodes.
}
If \code{return.first = TRUE}, the output will be a list containing \code{$first} and \code{$last}:
#' \itemize{
\item \code{$first} contains the first DAG, in a minimal form, which means that it is not a Bayesian Network but still retains a
structure that allows the usage of plot.DBN() to examine the graph.
\item \code{$last} contains the proper, ready to use, Bayesian Network.
}

\strong{Aditional details}
\code{output.marginals} and \code{compile.junction} are useful to save time if the user only intends to visualize the DAG.
\code{whitelist} and \code{blacklist} arguments can be passed to structure.learning.args.list, but beware of the naming convention,
it is best to use plotDBN() first with a dummy network.
}
\examples{
# Loading predictors
}
\author{
MN Legasa
}
